---
layout : page
title : book and courses I would recommend
---

Here is a list of textbooks that I have found to be valuble.

## Neuroscience (mostly computational)
* Theoretical Neuroscience by Dayan and Abbott
  * The quintessential computational neuroscience textbook. The first part discusses information-theory/signal-processing methods for analyzing neural spike trains, the second part is about biophysical models of single-neurons and (biological) networks, and the third part is about synaptic plasticity and learning rules. It's very math-heavy, so having a good understanding of calculus, probability and linear algebra, and some basic knowledge of electrophysics, signal-processing and differential equations is definitely helpful. 
* Neuronal Dynamics by Gerstner, Kistler, Naud and Paninski
  * It has a free online textbook [link](https://neuronaldynamics.epfl.ch/)
  * Similar, but a more accessible alternative to Dayan and Abbott. 
* Principles of Neural Design by Laughin and Sterling
  * Not a traditional neuroscience textbook. Discusses the biology of the brain from an engineering/design perspective, particularly in regards to optimizing energy and space constraints. 21st-century neuroscience's answer to "On Growth and Form". 
* Neural Networks and Brain Function by Rolls and Treves
  * This is not an introductory textbook on the field of computational neuroscience, but a description of the author's theories on how neural network architectures could be actually implemented in the brain. Long and short-term potentiation dynamics of specialized cell types, anatomical connectivity, and behavioral evidence are invoked to show the types of computation that can be performed by different brain regions. It's slightly outdated now, but contains many useful references for anatomically-precise model building. 
  * Edit: Apparently there is a book by the same author on a similar topic published only last year. [link](https://www.oxcns.org/b16text.html)

## Math/CS/Machine learning
* Introduction to the theory of neural computation by Hertz, Krough and Palmer
  * Possibly the best introductory text on non-deep-learning neural network theory. Hopfield nets, perceptrons, Boltzmann machines, competitive learning (e.g. Kohonen maps), some early work with backprop.
* Deep Learning by Goodfellow, Bengio and Courville
  * The definitive textbook for modern deep learning. Regularization and optimization methods, analysis of gradients, convolutional NN's, recurrent NN's, autoencoders, generative models, etc. 
* The Matrix Cookbook by Petersen and Pedersen
  * A list of formulas and identities, given without proof. There are very few textbooks and courses on matrix calculus, since prior to deep neural networks there have rarely been circumstances in which you would need to take a derivative with respect to a matrix. 
  * There is an MIT course on matrix calculus based on this book [link](https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/)
* Pattern Recognition and Machine Learning by Christopher Bishop
  * "Classical" machine learning, presented with mathematical rigour. Regression analysis, kernel methods, SVM, PCA, Markov models, Expectation Maximization, Bayesian inference, etc. 
* Introduction to Real Analysis by Rudin


## Not-textbooks
* [Scholarpedia](http://www.scholarpedia.org/article/Main_Page) has a lot of good articles on dynamical systems, machine learning, and computational neuroscience. 